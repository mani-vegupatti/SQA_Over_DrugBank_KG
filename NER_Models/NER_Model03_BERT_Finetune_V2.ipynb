{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_Model03_BERT_Finetune_V2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGnFcfmxhFvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "376a2128-8cec-4aa5-a9c4-94b4c04f5fcf"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNwFtQLfh2oW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cfdfb715-187a-4cf3-a96f-66382f9ee241"
      },
      "source": [
        "# set google drive for files\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/drive/My Drive/Colab Notebooks/temp/b08d5871a151.json\"\n",
        "!echo $GOOGLE_APPLICATION_CREDENTIALS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/temp/b08d5871a151.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndGmJx-nh3Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set path for files\n",
        "path = \"/content/drive/My Drive/thesis_dataset/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2My7CPyhhFGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "e0f780c2-aaa7-43c2-e3a6-b12392754ff0"
      },
      "source": [
        "# install required packages\n",
        "!pip install transformers\n",
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.2.4->seqeval) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cGAEKO1h6-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "47f1b232-7ef1-45c2-a113-ef3022db3f12"
      },
      "source": [
        "# import required packages/modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "import csv\n",
        "\n",
        "print(f'Torch: {torch.__version__}, Transformers: {transformers.__version__}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch: 1.6.0+cu101, Transformers: 3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41mAaRxRFgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define constants\n",
        "MAX_TOKENS = 64\n",
        "BATCH_SIZE = 32\n",
        "NR_EPOCHES = 20\n",
        "MAX_NORM = 1.0\n",
        "BERT_PRETRAIN_MODEL_NAME = \"bert-base-cased\"\n",
        "NR_WARM_STEPS = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGxwV6fJh9p5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "7e77617b-f59a-4525-fc18-1ad658991e94"
      },
      "source": [
        "# read data from excel\n",
        "df = pd.read_excel(path+\"All_Questions_V1.xlsx\",'data', encoding='utf-8') \n",
        "df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SlNo</th>\n",
              "      <th>Question</th>\n",
              "      <th>Relation</th>\n",
              "      <th>NER_Tag</th>\n",
              "      <th>Q_Len</th>\n",
              "      <th>T_Len</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Subject_URI</th>\n",
              "      <th>Relation_URI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>what are the brand names of Metipranolol</td>\n",
              "      <td>brand</td>\n",
              "      <td>O O O O O O B-E</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>Metipranolol</td>\n",
              "      <td>http://bio2rdf.org/drugbank:DB01214</td>\n",
              "      <td>http://bio2rdf.org/drugbank_vocabulary:brand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SlNo  ...                                  Relation_URI\n",
              "0     1  ...  http://bio2rdf.org/drugbank_vocabulary:brand\n",
              "\n",
              "[1 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZtLr2gOqy_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "02b2ba92-43dc-4023-caa0-960ca842a879"
      },
      "source": [
        "# split the full dataset into train, valid and test dataset\n",
        "rest, test = train_test_split(df, test_size=0.2, random_state=0, \n",
        "                               stratify=df['Relation'])\n",
        "train, valid = train_test_split(rest, test_size=0.1, random_state=0, \n",
        "                               stratify=rest['Relation'])\n",
        "print(f'Train:{len(train)}, Test: {len(test)}, Validation: {len(valid)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train:406, Test: 114, Validation: 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmFJoiioRPuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b7e4248-ba9e-4fe7-bbd1-b829cb635e92"
      },
      "source": [
        "# make the processing device as GPU or CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMpcU7SYQsrB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "972a3f16-634f-403a-bb19-36a12e26e348"
      },
      "source": [
        "# create dictionary of NER_TAGs\n",
        "tag_ids = ['O', 'B-E', 'I-E', 'PAD']\n",
        "tag_dict = {t: i for i, t in enumerate(tag_ids)}\n",
        "num_ner_tags = len(tag_dict)\n",
        "print(num_ner_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCbD1wVlQ8IW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create instance of tokenzier from BERT pretrained model\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_PRETRAIN_MODEL_NAME, do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_fcWYWDRhb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# process the question phrase to return tokens list\n",
        "# process the NER_TAGs to match wordpieces of tokenizer\n",
        "def process_tokens_labels(sent, labels):\n",
        "    tokens_list = []\n",
        "    labels_list = []\n",
        "    for word, label in zip(sent, labels):\n",
        "        # process tokens\n",
        "        tokens = tokenizer.tokenize(word)\n",
        "        tokens_list.extend(tokens)\n",
        "        # process labels\n",
        "        num_wordpieces = len(tokens)\n",
        "        labels_list.extend([label] * num_wordpieces)\n",
        "    return tokens_list, labels_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW2WCqa8QDLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# process the question phrase to return Torch tensors\n",
        "# process the NER_TAGs to retun Torch tensors\n",
        "def process_data(df_data):\n",
        "  # process input data\n",
        "  words_list = [[word for word in sentence.split()] for sentence in df_data['Question'].values]\n",
        "  print(words_list[0])\n",
        "  labels_list = [[tag for tag in tag_value.split()] for tag_value in df_data['NER_Tag'].values]\n",
        "  print(labels_list[0])\n",
        "  # gets inputs_ids and attention masks\n",
        "  tokens_with_labels = [process_tokens_labels(sentence, labels)\n",
        "                                for sentence, labels in zip(words_list, labels_list)]\n",
        "  tokens_list = [token_with_label[0] for token_with_label in tokens_with_labels]\n",
        "  input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(token) for token in tokens_list], maxlen=MAX_TOKENS,  \n",
        "                            truncating=\"post\", padding=\"post\", value=0.0, dtype=\"long\")\n",
        "  attn_masks = [[float(id != 0.0) for id in input_id] for input_id in input_ids]\n",
        "  # process labels and convert to numbers\n",
        "  new_labels_list = [token_with_label[1] for token_with_label in tokens_with_labels]\n",
        "  target_labels = pad_sequences([[tag_dict[lab] for lab in label] for label in new_labels_list], maxlen=MAX_TOKENS, \n",
        "                       truncating=\"post\", padding=\"post\", value=tag_dict[\"PAD\"], dtype=\"long\",)\n",
        "  \n",
        "  return torch.tensor(input_ids), torch.tensor(attn_masks), torch.tensor(target_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EegBf0sXQAS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "9348f573-b8b5-4f55-9b0c-9a0d7024eb5a"
      },
      "source": [
        "# process question phrases and NER_TAGs to get Torch tensors\n",
        "train_input_ids, train_attn_masks, train_ner_tags  = process_data(train)\n",
        "valid_input_ids, valid_attn_masks, valid_ner_tags  = process_data(valid)\n",
        "test_input_ids, test_attn_masks, test_ner_tags  = process_data(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['which', 'life', 'forms', 'are', 'impacted', 'by', 'Marimastat']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'B-E']\n",
            "['what', 'is', 'the', 'volume', 'of', 'distribution', 'for', 'Coagulation', 'factor', 'VIIa']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E']\n",
            "['Nitroglycerin', 'is', 'patented', 'under', 'which', 'number']\n",
            "['B-E', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J5YBCNFlXtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Process question phrases and NER_Tags to get datloader\n",
        "train_dataset = TensorDataset(train_input_ids, train_attn_masks, train_ner_tags)\n",
        "train_random_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_random_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_dataset = TensorDataset(valid_input_ids, valid_attn_masks, valid_ner_tags)\n",
        "valid_random_sampler = SequentialSampler(valid_dataset)\n",
        "valid_dataloader = DataLoader(valid_dataset, sampler=valid_random_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_attn_masks, test_ner_tags)\n",
        "test_random_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_random_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50FfBHF8lgJu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "cc4a929c-0ee4-4aab-9222-475222ae3d05"
      },
      "source": [
        "# create model from pretrained BERT model\n",
        "# send the model parameters to default device\n",
        "model = BertForTokenClassification.from_pretrained( BERT_PRETRAIN_MODEL_NAME , num_labels=num_ner_tags, output_attentions = False,output_hidden_states = False)\n",
        "model.cuda();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaJnYrcbxde2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretrained BERT base layers are also finetuned and we use lower learning rate \n",
        "# determine all parameters of all pretrained layers of the model and create optimizer\n",
        "parameters = list(model.named_parameters())\n",
        "optimizer_parameters = [{\"params\": [parameter for num, parameter in parameters]}]\n",
        "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "\n",
        "# Determine training steps and create scheduler\n",
        "train_steps = len(train_dataloader) * NR_EPOCHES\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, NR_WARM_STEPS, train_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6tQvuTwJqpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to update the parameters during training\n",
        "def model_training(train_dataloader):\n",
        "    # model in training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    # train the model and update parameters of all layers\n",
        "    for train_instance in train_dataloader:\n",
        "        train_data_row = tuple(row.to(device) for row in train_instance)\n",
        "        input_ids, attn_mask, labels = train_data_row\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attn_mask, labels=labels)\n",
        "        instance_loss = outputs[0]\n",
        "        instance_loss.backward()\n",
        "        train_loss += instance_loss.item()\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_NORM)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    return train_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSxPut-rUszh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to calculate evaluation metrics during validation\n",
        "def model_validation(valid_dataloader):\n",
        "    # model in evaluation mode\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    pred_labels = [] \n",
        "    act_labels = []\n",
        "    tokens = []\n",
        " \n",
        "    # find predicted NER_TAGs and retrieve actual NER_TAGs from tensor\n",
        "    for valid_instance in valid_dataloader:\n",
        "        valid_data = tuple(row.to(device) for row in valid_instance)\n",
        "        input_ids, attn_mask, labels = valid_data\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attn_mask, labels=labels)\n",
        "        instance_loss = outputs[0]\n",
        "        valid_loss += instance_loss.item()\n",
        "\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        pred_labels.extend([list(p_labels) for p_labels in np.argmax(logits, axis=2)])\n",
        "        act_label = labels.to('cpu').numpy()\n",
        "        act_labels.extend(act_label)\n",
        "\n",
        "        for input_id in input_ids:\n",
        "          tokens.extend([tokenizer.convert_ids_to_tokens(input_id.to('cpu').numpy())])\n",
        "\n",
        "    return valid_loss, pred_labels, act_labels, tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txfWge5sl2LA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67e3a7cc-7b01-46ad-f699-f5c56d0faa00"
      },
      "source": [
        "# train the model for required epoches \n",
        "for epoch_num in tqdm(range(NR_EPOCHES), desc=\"Training Progress\"):\n",
        "    num_train_samples = len(train_dataloader)\n",
        "    num_valid_samples = len(valid_dataloader)\n",
        "\n",
        "    train_loss = model_training(train_dataloader)\n",
        "    valid_loss, pred_labels, act_labels, _ = model_validation(valid_dataloader)\n",
        "\n",
        "    # calculate and print training loss\n",
        "    training_loss = train_loss / num_train_samples\n",
        "    print()\n",
        "    print(f'Training loss: {training_loss}')\n",
        "\n",
        "    # calculate and print validation loss, accuracy and F-Score\n",
        "    validation_loss = valid_loss / num_valid_samples\n",
        "    print(f'Validation loss: {validation_loss}')\n",
        "    pred_ner_tags = [tag_ids[pred] for pred_label, act_label in zip(pred_labels, act_labels)\n",
        "                                 for pred, act in zip(pred_label, act_label) if tag_ids[act] != \"PAD\"]\n",
        "    act_ner_tags = [tag_ids[act] for act_label in act_labels\n",
        "                                  for act in act_label if tag_ids[act] != \"PAD\"]\n",
        "    print(f'Validation Accuracy: {accuracy_score(pred_ner_tags, act_ner_tags)}')\n",
        "    print(f'Validation F-Score: {f1_score(pred_ner_tags, act_ner_tags)}')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Progress:   5%|▌         | 1/20 [00:03<00:57,  3.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.7828136109388791\n",
            "Validation loss: 0.458412230014801\n",
            "Validation Accuracy: 0.8364565587734242\n",
            "Validation F-Score: 0.7149321266968326\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  10%|█         | 2/20 [00:06<00:54,  3.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.25770169725784886\n",
            "Validation loss: 0.3284241482615471\n",
            "Validation Accuracy: 0.8960817717206133\n",
            "Validation F-Score: 0.7932692307692307\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  15%|█▌        | 3/20 [00:09<00:51,  3.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.12093467666552617\n",
            "Validation loss: 0.3407772034406662\n",
            "Validation Accuracy: 0.9148211243611585\n",
            "Validation F-Score: 0.8349514563106796\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  20%|██        | 4/20 [00:12<00:48,  3.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.06365664604191597\n",
            "Validation loss: 0.2238511461764574\n",
            "Validation Accuracy: 0.9557069846678024\n",
            "Validation F-Score: 0.8992248062015504\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  25%|██▌       | 5/20 [00:15<00:45,  3.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.021288384396869402\n",
            "Validation loss: 0.26278222166001797\n",
            "Validation Accuracy: 0.9676320272572402\n",
            "Validation F-Score: 0.9336734693877551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  30%|███       | 6/20 [00:18<00:42,  3.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.013857544202787371\n",
            "Validation loss: 0.25906841456890106\n",
            "Validation Accuracy: 0.9659284497444633\n",
            "Validation F-Score: 0.926208651399491\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  35%|███▌      | 7/20 [00:21<00:39,  3.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.00938059353771118\n",
            "Validation loss: 0.27143427170813084\n",
            "Validation Accuracy: 0.9642248722316865\n",
            "Validation F-Score: 0.9238578680203046\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  40%|████      | 8/20 [00:24<00:36,  3.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.006837617005937948\n",
            "Validation loss: 0.21341476030647755\n",
            "Validation Accuracy: 0.9744463373083475\n",
            "Validation F-Score: 0.9492385786802031\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  45%|████▌     | 9/20 [00:27<00:32,  3.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.0049143837442478305\n",
            "Validation loss: 0.34676104225218296\n",
            "Validation Accuracy: 0.9625212947189097\n",
            "Validation F-Score: 0.9187817258883249\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  50%|█████     | 10/20 [00:30<00:29,  3.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.003709298715246125\n",
            "Validation loss: 0.3010765574872494\n",
            "Validation Accuracy: 0.9625212947189097\n",
            "Validation F-Score: 0.9215189873417721\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  55%|█████▌    | 11/20 [00:33<00:26,  2.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.0029036627652553413\n",
            "Validation loss: 0.29866110160946846\n",
            "Validation Accuracy: 0.9659284497444633\n",
            "Validation F-Score: 0.9316455696202532\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  60%|██████    | 12/20 [00:36<00:24,  3.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.005248996218702255\n",
            "Validation loss: 0.36577668227255344\n",
            "Validation Accuracy: 0.9659284497444633\n",
            "Validation F-Score: 0.9312977099236641\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  65%|██████▌   | 13/20 [00:39<00:21,  3.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.0020497560169762718\n",
            "Validation loss: 0.3264639712870121\n",
            "Validation Accuracy: 0.969335604770017\n",
            "Validation F-Score: 0.9411764705882353\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  70%|███████   | 14/20 [00:42<00:17,  2.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.004078362439311325\n",
            "Validation loss: 0.28822724521160126\n",
            "Validation Accuracy: 0.969335604770017\n",
            "Validation F-Score: 0.9411764705882353\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  75%|███████▌  | 15/20 [00:45<00:14,  2.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.001904968387232377\n",
            "Validation loss: 0.3195117451250553\n",
            "Validation Accuracy: 0.9676320272572402\n",
            "Validation F-Score: 0.9336734693877551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  80%|████████  | 16/20 [00:48<00:11,  2.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.002954080371777169\n",
            "Validation loss: 0.34137845039367676\n",
            "Validation Accuracy: 0.9676320272572402\n",
            "Validation F-Score: 0.9336734693877551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  85%|████████▌ | 17/20 [00:51<00:08,  2.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.000720620195632084\n",
            "Validation loss: 0.34836279414594173\n",
            "Validation Accuracy: 0.9659284497444633\n",
            "Validation F-Score: 0.9312977099236641\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  90%|█████████ | 18/20 [00:53<00:05,  2.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.0008930886934439724\n",
            "Validation loss: 0.33920054510235786\n",
            "Validation Accuracy: 0.969335604770017\n",
            "Validation F-Score: 0.9411764705882353\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  95%|█████████▌| 19/20 [00:56<00:02,  2.95s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.000663855319054654\n",
            "Validation loss: 0.33615018613636494\n",
            "Validation Accuracy: 0.969335604770017\n",
            "Validation F-Score: 0.9411764705882353\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 20/20 [00:59<00:00,  3.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.001204238640359388\n",
            "Validation loss: 0.33433468267321587\n",
            "Validation Accuracy: 0.9676320272572402\n",
            "Validation F-Score: 0.9360613810741688\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohl7AowEVc9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to evaluate any given dataset\n",
        "def evaluate_model(dataloader):\n",
        "    num_valid_samples = len(dataloader)\n",
        "    calc_loss, pred_labels, act_labels, tokens = model_validation(dataloader)\n",
        "\n",
        "    # calculate and print validation loss, accuracy and F-Score\n",
        "    final_loss = calc_loss / num_valid_samples\n",
        "    print(f'Loss: {final_loss}')\n",
        "    pred_ner_tags = [tag_ids[pred] for pred_label, act_label in zip(pred_labels, act_labels)\n",
        "                                 for pred, act in zip(pred_label, act_label) if tag_ids[act] != \"PAD\"]\n",
        "    act_ner_tags = [tag_ids[act] for act_label in act_labels\n",
        "                                  for act in act_label if tag_ids[act] != \"PAD\"]\n",
        "    print(f'Accuracy: {accuracy_score(pred_ner_tags, act_ner_tags)}')\n",
        "    print(f'F-Score: {f1_score(pred_ner_tags, act_ner_tags)}')\n",
        "    print(classification_report(pred_ner_tags, act_ner_tags))    \n",
        "    print()\n",
        "\n",
        "    # reconstruct tokens, lables and entities\n",
        "    # print actual and predicted for visual comparision\n",
        "    p_labels_list, a_labels_list, tokens_list, a_entities_list, p_entities_list = [], [], [], [], []\n",
        "    for token, prd_label, act_label in zip(tokens, pred_labels, act_labels ):\n",
        "      new_p_labels, new_a_labels, new_tokens = [], [], []\n",
        "      a_entity, p_entity = \"\", \"\"\n",
        "      a_done_flag, p_done_flag = False, False\n",
        "      a_inside_flag, p_inside_flag = False, False\n",
        "      a_prev_tag, p_prev_tag = 'O', 'O'\n",
        "      for token, label_idx, t_label_idx in zip(token, prd_label, act_label):\n",
        "        if t_label_idx != 3:\n",
        "          if token.startswith(\"##\"):\n",
        "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "\n",
        "            if not(a_done_flag) and a_inside_flag:\n",
        "              a_entity += token[2:]\n",
        "\n",
        "            if not(p_done_flag) and p_inside_flag:\n",
        "              p_entity += token[2:]\n",
        "          else:\n",
        "            new_p_labels.append(tag_ids[label_idx])\n",
        "            new_a_labels.append(tag_ids[t_label_idx])\n",
        "            new_tokens.append(token)\n",
        "\n",
        "            a_curnt_tag = tag_ids[t_label_idx]\n",
        "            if not(a_done_flag):\n",
        "              if a_curnt_tag in ['B-E', 'I-E']:\n",
        "                if token not in [\"'\", \"s\"]:\n",
        "                  if token == \"-\" or a_entity[-1:] == \"-\":\n",
        "                    a_entity = a_entity+ token\n",
        "                  elif a_entity == \"\":\n",
        "                    a_entity = token\n",
        "                  else:\n",
        "                    a_entity = a_entity+ \" \" +token\n",
        "                  a_inside_flag = True\n",
        "              else:\n",
        "                if a_prev_tag in ['B-E', 'I-E']:\n",
        "                  a_done_flag = True\n",
        "            \n",
        "            p_curnt_tag = tag_ids[label_idx]\n",
        "            if not(p_done_flag):\n",
        "              if p_curnt_tag in ['B-E', 'I-E']:\n",
        "                if token not in [\"'\", \"s\"]:\n",
        "                  if token == \"-\" or p_entity[-1:] == \"-\":\n",
        "                    p_entity = p_entity+ token\n",
        "                  elif p_entity == \"\":\n",
        "                    p_entity = token\n",
        "                  else:\n",
        "                    p_entity = p_entity+ \" \" +token\n",
        "                  p_inside_flag = True\n",
        "              else:\n",
        "                if p_prev_tag in ['B-E', 'I-E']:\n",
        "                  p_done_flag = True\n",
        "        \n",
        "      tokens_list.append(new_tokens) \n",
        "      p_labels_list.append(new_p_labels)    \n",
        "      a_labels_list.append(new_a_labels)\n",
        "      a_entities_list.append(a_entity)\n",
        "      p_entities_list.append(p_entity)\n",
        "\n",
        "    print(\"Tokens List\")\n",
        "    print(tokens_list)\n",
        "    print(\"Predicted Labels\")\n",
        "    print(p_labels_list)\n",
        "    print(\"Actual Labels\")\n",
        "    print(a_labels_list)\n",
        "    print(\"Predicted Entities\")\n",
        "    print(p_entities_list)\n",
        "    print(\"Actual Entities\")\n",
        "    print(a_entities_list)\n",
        "\n",
        "    # write the predicted entity strings to csv file\n",
        "    with open(path+'test_entities_final_v0.csv', 'w', newline='') as myfile:\n",
        "      wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
        "      wr.writerow(p_entities_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwwecQ9fYdQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "9b1c53ca-4684-4951-a816-e79829f19b35"
      },
      "source": [
        "print(f'                 Validation Dataset Results                  ')\n",
        "print(\"--------------------------------------------------------------\")\n",
        "evaluate_model(valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 Validation Dataset Results                  \n",
            "--------------------------------------------------------------\n",
            "Loss: 0.33433468267321587\n",
            "Accuracy: 0.9676320272572402\n",
            "F-Score: 0.9360613810741688\n",
            "           precision    recall  f1-score   support\n",
            "\n",
            "        E       0.96      0.92      0.94       200\n",
            "\n",
            "micro avg       0.96      0.92      0.94       200\n",
            "macro avg       0.96      0.92      0.94       200\n",
            "\n",
            "\n",
            "Tokens List\n",
            "[['what', 'is', 'the', 'volume', 'of', 'distribution', 'for', 'Coagulation', 'factor', 'VIIa'], ['what', 'is', 'exact', 'the', 'position', 'of', 'Ferritin', 'heavy', 'chain', 'on', 'a', 'chromosome'], ['how', 'the', 'interaction', 'of', 'Dronedarone', 'affects', 'other', 'drugs', \"'\", 's', 'actions'], ['which', 'is', 'the', 'kingdom', 'grouping', 'of', 'the', 'drug', 'Alpha', '-', 'Linolenic', 'Acid'], ['provide', 'the', 'general', 'activities', 'carried', 'out', 'by', 'Protein', 'S100', '-', 'A1'], ['Leukotriene', 'C4', 'synthase', 'is', 'encoded', 'by', 'which', 'gene'], ['how', 'Nandrolone', 'decanoate', 'is', 'metabolised'], ['provide', 'the', 'patent', 'number', 'filed', 'for', 'Mirabegron'], ['what', 'are', 'the', 'foods', 'to', 'consume', 'and', 'avoid', 'when', 'taking', 'Aminoglutethimide'], ['Quinapril', 'is', 'content', 'of', 'which', 'mixtures'], ['Acetaminophen', 'present', 'in', 'which', 'mixture'], ['provide', 'the', 'name', 'of', 'packaging', 'company', 'for', 'Diltiazem'], ['what', 'are', 'the', 'other', 'well', 'known', 'names', 'of', 'Lincomycin'], ['provide', 'the', 'active', 'drug', 'removal', 'rate', 'for', 'Rocuronium'], ['what', 'are', 'the', 'possible', 'organisms', 'impacted', 'by', 'Insulin', 'Glargine'], ['how', 'much', 'of', 'Allopurinol', 'is', 'removed', 'during', 'elimination', 'process'], ['what', 'are', 'the', 'current', 'status', 'groups', 'of', 'Estrone'], ['what', 'is', 'the', 'value', 'of', 'locus', 'for', 'Egl', 'nine', 'homolog', '1'], ['Nitrofurantoin', 'falls', 'under', 'which', 'category'], ['how', 'much', 'volume', 'of', 'Clomipramine', 'is', 'distributed', 'after', 'administration', 'in', 'body', 'fluids'], ['which', 'prescription', 'product', 'forms', 'are', 'available', 'for', 'Nizatidine'], ['which', 'molecule', 'and', 'function', 'is', 'targeted', 'by', 'Pyridoxal', 'Phosphate'], ['provide', 'the', 'actual', 'pi', 'value', 'of', 'Endo', '-', 'N', '-', 'acetylneuraminidase'], ['Photoactive', 'yellow', 'protein', 'is', 'involved', 'in', 'which', 'basic', 'functions'], ['Cycrimine', 'is', 'grouped', 'into', 'which', 'kingdom', 'of', 'compounds'], ['provide', 'molecular', 'weight', 'of', 'enzyme', 'Aldehyde', 'oxidase'], ['N', '-', 'Cholylglycine', \"'\", 's', 'efflux', 'is', 'carried', 'out', 'by', 'which', 'transporter'], ['provide', 'protein', 'binding', 'value', 'of', 'Dofetilide'], ['list', 'the', 'all', 'known', 'side', 'effects', 'on', 'using', 'Levetiracetam'], ['Penbutolol', 'is', 'used', 'in', 'which', 'end', 'products'], ['what', 'are', 'the', 'details', 'in', 'the', 'pharmacology', 'report', 'of', 'Botulinum', 'Toxin', 'Type', 'A'], ['name', 'the', 'brands', 'for', 'PA', 'mAb'], ['who', 'produces', 'Penicillin', 'V'], ['list', 'the', 'specific', 'functions', 'carried', 'out', 'by', 'Exotoxin', 'A'], ['Growth', 'hormone', 'receptor', 'is', 'available', 'in', 'which', 'organisms'], ['explain', 'the', 'working', 'of', 'Stanozolol', 'for', 'inducing', 'the', 'required', 'results'], ['what', 'is', 'the', 'unit', 'dose', 'form', 'of', 'Pentosan', 'Polysulfate'], ['list', 'the', 'salts', 'of', 'Grepafloxacin'], ['what', 'are', 'the', 'indications', 'treated', 'using', 'Sibutramine'], ['at', 'which', 'location', 'of', 'a', 'cell', 'we', 'can', 'find', 'concentrations', 'of', 'Protein', 'UshA'], ['how', 'much', 'Pseudoephedrine', 'binds', 'to', 'protein'], ['what', 'is', 'Peginterferon', 'alfa', '-', '2a', \"'\", 's', 'half', 'life', 'value'], ['provide', 'Sodium', 'Tetradecyl', 'Sulfate', \"'\", 's', 'structure', 'name'], ['recombinant', 'human', 'GM', '-', 'CSF', 'is', 'helpful', 'in', 'treating', 'which', 'signs', 'and', 'symptoms'], ['which', 'gene', 'can', 'be', 'helpful', 'to', 'produce', 'Protein', 'Nef'], ['what', 'are', 'Marvelon', '21', 'Tab', \"'\", 's', 'ingredients']]\n",
            "Predicted Labels\n",
            "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-E', 'I-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'I-E', 'I-E'], ['B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'B-E', 'B-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['B-E', 'B-E', 'B-E', 'B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O', 'O', 'O'], ['O', 'B-E', 'B-E', 'I-E', 'I-E', 'I-E', 'O', 'O'], ['O', 'O', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O']]\n",
            "Actual Labels\n",
            "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-E', 'I-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'B-E', 'B-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['B-E', 'B-E', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O', 'O', 'O'], ['O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O', 'O'], ['B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O']]\n",
            "Predicted Entities\n",
            "['Coagulation factor VIIa', 'Ferritin heavy chain', 'Dronedarone', 'Alpha-Linolenic Acid', 'Protein S100-A1', 'Leukotriene C4ynthase', 'Nandrolone decanoatebolised', 'Mirabegron', 'Aminoglutethimide', 'Quinaprils', 'Acetaminophen', 'Diltiazem', 'Lincomycin', 'Rocuronium', 'Insulin Glargine', 'Allopurinol', 'Estrone', 'Egl nine homolog 1', 'Nitrofurantoin', 'Clomipramine', 'Nizatidine', 'Pyridoxal Phosphate', 'Endo-N-acetylneuraminidase', 'Photoactive yellow protein', 'Cycrimine', 'Aldehyde oxidase', 'N-Cholylglycine effluxer', 'Dofetilide', 'Levetiracetam', 'Penbutolol', 'Botulinum Toxin Type A', 'PA mAb', 'Penicillin V', 'Exotoxin A', 'Growth hormone receptor', 'Stanozololducing', 'Pentosan Polysulfate', 'Grepafloxacin', 'Sibutramine', 'Protein UshA', 'Pseudoephedrine', 'Peginterferon alfa-2a', 'Sodium Tetradecyl Sulfate', 'GM-CSF', 'Protein Nef', 'Marvelon 21 Tab']\n",
            "Actual Entities\n",
            "['Coagulation factor VIIa', 'Ferritin heavy chain', 'Dronedarone', 'Alpha-Linolenic Acid', 'Protein S100-A1', 'Leukotriene C4ynthase', 'Nandrolone decanoatebolised', 'Mirabegron', 'Aminoglutethimide', 'Quinaprils', 'Acetaminophen', 'Diltiazem', 'Lincomycin', 'Rocuronium', 'Insulin Glargine', 'Allopurinol', 'Estrone', 'Egl nine homolog 1', 'Nitrofurantoin', 'Clomipramine', 'Nizatidine', 'Pyridoxal Phosphate', 'Endo-N-acetylneuraminidase', 'Photoactive yellow protein', 'Cycrimine', 'Aldehyde oxidase', 'N-Cholylglycineffluxer', 'Dofetilide', 'Levetiracetam', 'Penbutolol', 'Botulinum Toxin Type A', 'PA mAb', 'Penicillin V', 'Exotoxin A', 'Growth hormone receptor', 'Stanozololducing', 'Pentosan Polysulfate', 'Grepafloxacin', 'Sibutramine', 'Protein UshA', 'Pseudoephedrine', 'Peginterferon alfa-2a', 'Sodium Tetradecyl Sulfate', 'recombinant human GM-CSF', 'Protein Nef', 'Marvelon 21 Tab']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6qTP_g8Z0p_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "c1cb676b-65c4-4fd7-8e37-88ae12152e8a"
      },
      "source": [
        "print(f'                 Test Dataset Results                  ')\n",
        "print(\"--------------------------------------------------------------\")\n",
        "evaluate_model(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 Test Dataset Results                  \n",
            "--------------------------------------------------------------\n",
            "Loss: 0.11270354269072413\n",
            "Accuracy: 0.9806666666666667\n",
            "F-Score: 0.954864593781344\n",
            "           precision    recall  f1-score   support\n",
            "\n",
            "        E       0.97      0.94      0.95       507\n",
            "\n",
            "micro avg       0.97      0.94      0.95       507\n",
            "macro avg       0.97      0.94      0.95       507\n",
            "\n",
            "\n",
            "Tokens List\n",
            "[['Nitroglycerin', 'is', 'patented', 'under', 'which', 'number'], ['which', 'companies', 'manufacture', 'Phenmetrazine'], ['list', 'all', 'synonyms', 'of', 'Nepafenac'], ['list', 'the', 'mixtures', 'that', 'contains', 'Hydrochlorothiazide'], ['which', 'is', 'the', 'transporter', 'for', 'Zafirlukast'], ['what', 'are', 'the', 'overdose', 'impacts', 'of', 'Theophylline'], ['what', 'is', 'the', 'general', 'function', 'of', 'enzme', 'Aldehyde', 'oxidase'], ['for', 'Vitamin', 'D3', 'receptor', 'provide', 'actual', 'isoelectric', 'point', 'value'], ['provide', 'the', 'kingdom', 'name', 'of', 'Glutethimide'], ['provide', 'the', 'statuses', 'for', 'L', '-', 'Cysteine'], ['which', 'diseases', 'are', 'treated', 'using', 'Flurandrenolide'], ['provide', 'the', 'details', 'of', 'pharmacology', 'study', 'for', 'Dipivefrin'], ['Flavohemoprotein', 'is', 'based', 'on', 'which', 'gene'], ['the', 'actions', 'of', 'Preotact', 'focuses', 'on', 'which', 'targets'], ['what', 'are', 'the', 'synonyms', 'of', 'MGCD', '-', '0103'], ['what', 'is', 'the', 'general', 'function', 'of', 'enzme', 'Cytochrome', 'P450', '2B7', 'isoform'], ['what', 'is', 'the', 'registered', 'patent', 'number', 'of', 'Deferasirox'], ['which', 'cellular', 'location', 'we', 'can', 'find', 'enzyme', 'Aldehyde'], ['what', 'is', 'the', 'elimination', 'process', 'of', 'Ibuprofen', 'in', 'the', 'body'], ['provide', 'general', 'function', 'of', 'Transthyretin'], ['what', 'is', 'the', 'structure', 'name', 'of', 'Cefmetazole'], ['what', 'is', 'the', 'category', 'of', 'Glycine'], ['list', 'the', 'known', 'undesirable', 'side', 'effects', 'of', 'Hydromorphone'], ['Fluticasone', 'Propionate', 'is', 'protected', 'using', 'which', 'patent', 'number'], ['list', 'the', 'prescription', 'product', 'forms', 'of', 'Ketoconazole'], ['which', 'compound', 'structure', 'does', 'Phenylephrine', 'belongs', 'to'], ['which', 'salt', 'form', 'is', 'produced', 'from', 'Estriol'], ['what', 'are', 'the', 'generic', 'functionalities', 'performed', 'by', 'Acetyl', '-', 'CoA', 'carboxylase', '2'], ['provide', 'the', 'estimated', 'half', 'life', 'for', 'Fusidic', 'Acid'], ['current', 'status', 'of', 'Simvastatin', 'falls', 'under', 'which', 'groups'], ['provide', 'the', 'available', 'brands', 'for', 'Peginterferon', 'alfa', '-', '2b'], ['Loratadine', 'is', 'administered', 'to', 'treat', 'which', 'diseases'], ['what', 'is', 'the', 'working', 'mechanism', 'of', 'NBI', '-', '6024'], ['what', 'are', 'the', 'other', 'equivalent', 'names', 'used', 'for', 'Cephalexin'], ['AZD6140', 'affects', 'which', 'organisms'], ['what', 'is', 'the', 'gene', 'name', 'of', 'Vitamin', 'D', '-', 'binding', 'protein'], ['how', 'much', 'of', 'Warfarin', 'can', 'be', 'safely', 'distibuted', 'in', 'body', 'water', 'by', 'volume'], ['list', 'the', 'organisms', 'in', 'which', 'the', 'effect', 'of', 'Pegaptanib', 'is', 'observed'], ['provide', 'the', 'commercial', 'products', 'that', 'contain', 'Human', 'Serum', 'Albumin'], ['name', 'of', 'the', 'conditions', 'for', 'which', 'MEM', '1414', 'is', 'prescribed'], ['what', 'are', 'the', 'various', 'volume', 'of', 'distribution', 'of', 'Delorazepam', 'in', 'patients'], ['which', 'mixtures', 'are', 'produced', 'using', 'Menthol'], ['what', 'is', 'the', 'locus', 'value', 'of', 'enzyme', 'Cytochrome', 'P450', '2B7', 'isoform'], ['what', 'are', 'all', 'the', 'mixtures', 'made', 'with', 'Chlorphenamine'], ['Diclofenac', 'is', 'packaged', 'and', 'distributed', 'by', 'which', 'companies'], ['how', 'long', 'it', 'takes', 'for', 'Venlafaxine', 'to', 'reduce', 'its', 'concentration', 'by', 'one', 'half'], ['what', 'is', 'the', 'molecular', 'mass', 'value', 'of', 'Hydrolase'], ['list', 'the', 'constituents', 'of', 'Gold', 'bond', 'medicated', 'lotion'], ['what', 'are', 'the', 'specific', 'functions', 'carried', 'out', 'by', 'Beta', '-', 'lactamase', 'TEM'], ['list', 'drug', 'to', 'drug', 'interaction', 'for', 'Isradipine'], ['33', 'kDa', 'chaperonin', 'is', 'present', 'in', 'which', 'location', 'of', 'a', 'cell'], ['provide', 'the', 'value', 'of', 'Ammonia', 'channel', \"'\", 's', 'molecular', 'weight'], ['how', 'much', 'Moxifloxacin', 'is', 'bound', 'to', 'various', 'protein', 'types'], ['in', 'which', 'organism', 'we', 'can', 'find', 'the', 'enzyme', 'Aldehyde', 'oxidase'], ['in', 'which', 'dosage', 'form', 'Ziprasidone', 'is', 'administrated'], ['in', 'which', 'organism', 'we', 'can', 'find', 'the', 'enzyme', 'Cytochrome', 'P450', '2B7', 'isoform'], ['provide', 'locus', 'value', 'of', 'Thyroxine', '-', 'binding', 'globulin'], ['what', 'is', 'the', 'value', 'estimated', 'for', 'volume', 'of', 'distribution', 'for', 'Lepirudin'], ['which', 'company', 'produces', 'the', 'medicine', 'Clobetasol', 'propionate'], ['which', 'protein', 'helps', 'the', 'transportation', 'of', 'Ioflupane', 'I', '123', 'across', 'membrane'], ['how', 'much', 'is', 'the', 'molecular', 'weight', 'of', 'Subtilisin', 'Savinase'], ['how', 'Oxiconazole', 'acts', 'to', 'produce', 'desired', 'effects'], ['what', 'is', 'the', 'given', 'theoretical', 'pi', 'value', 'for', 'Vitamin', 'D', '-', 'binding'], ['what', 'are', 'the', 'different', 'forms', 'of', 'products', 'sold', 'for', 'Timolol'], ['provide', 'the', 'routes', 'through', 'which', 'Dobutamine', 'is', 'removed'], ['tell', 'me', 'the', 'specific', 'function', 'of', 'Thyroxine', '-', 'binding', 'globulin'], ['detail', 'the', 'actions', 'of', 'Pergolide', 'as', 'provided', 'in', 'the', 'pharmacology', 'reports'], ['which', 'specific', 'functions', 'are', 'performed', 'by', 'Peptide', 'deformylase'], ['which', 'targets', 'are', 'activated', 'by', 'Icosapent'], ['Dihomo', '-', 'γ', '-', 'linolenic', 'acid', 'is', 'avilable', 'by', 'which', 'brand', 'names'], ['at', 'which', 'locus', 'point', 'does', 'Lactotransferrin', 'present'], ['Lindane', 'is', 'classified', 'into', 'which', 'status', 'groups', 'of', 'drugs'], ['Denileukin', 'diftitox', 'is', 'cleared', 'from', 'system', 'with', 'what', 'rate'], ['provide', 'the', 'name', 'of', 'companies', 'that', 'pack', 'and', 'supply', 'Metronidazole'], ['what', 'is', 'the', 'biotransformation', 'process', 'of', 'Axitinib'], ['provide', 'the', 'name', 'of', 'the', 'transporter', 'of', 'Zidovudine'], ['which', 'gene', 'defines', 'Arylamine', 'N', '-', 'acetyltransferase'], ['provide', 'all', 'items', 'used', 'as', 'ingredient', 'for', 'Bronchyl', 'SYR'], ['list', 'Aclidinium', \"'\", 's', 'adverse', 'impacts', 'on', 'patients'], ['list', 'the', 'dose', 'forms', 'possible', 'for', 'use', 'of', '19', '-', 'norandrostenedione'], ['what', 'is', 'the', 'estimated', 'clearance', 'rate', 'values', 'for', 'Adefovir', 'Dipivoxil'], ['what', 'is', 'the', 'locus', 'position', 'of', 'Glutathione', 'peroxidase', '1', 'in', 'a', 'chromosome'], ['Chlorprothixene', 'is', 'eliminated', 'in', 'how', 'many', 'hours', 'of', 'half', 'life'], ['what', 'are', 'the', 'available', 'brand', 'names', 'for', 'Tauroursodeoxycholic', 'acid'], ['Lin', 'o', 'gel', 'is', 'made', 'using', 'which', 'ingredients'], ['provide', 'the', 'food', 'interaction', 'details', 'for', 'Ibandronate'], ['Flavopiridol', 'is', 'available', 'in', 'which', 'salt', 'forms'], ['explain', 'about', 'other', 'drugs', 'interaction', 'process', 'details', 'of', 'Magnesium', 'oxide'], ['for', 'Aldehyde', 'oxidase', 'provide', 'the', 'value', 'of', 'theoretical', 'pi'], ['DNA', 'polymerase', 'I', 'is', 'part', 'of', 'which', 'organisms'], ['provide', 'the', 'company', 'name', 'which', 'manufactures', 'Codeine'], ['provide', 'elimination', 'route', 'and', 'time', 'for', 'Clopidogrel'], ['what', 'is', 'salt', 'name', 'in', 'which', 'Ketobemidone', 'available', 'for', 'use'], ['how', 'much', 'of', 'Gadofosveset', 'trisodium', 'is', 'eliminated', 'during', 'a', 'treatment', 'in', 'unit', 'time'], ['what', 'is', 'mechanism', 'of', 'L', '-', 'Phenylalanine', 'while', 'performing', 'the', 'required', 'activities'], ['which', 'conditions', 'are', 'managed', 'with', 'help', 'of', 'Sodium', 'bicarbonate'], ['how', 'other', 'drugs', 'interact', 'with', 'drug', 'Isosorbide', 'Dinitrate'], ['in', 'which', 'forms', 'dosage', 'can', 'be', 'administrated', 'for', 'Glucagon', 'recombinant'], ['what', 'is', 'the', 'kingdom', 'of', 'Cinoxacin'], ['who', 'are', 'the', 'packagers', 'of', 'Potassium', 'Chloride'], ['what', 'are', 'the', 'known', 'food', 'interactions', 'of', 'the', 'drug', 'Cefuroxime'], ['what', 'range', 'of', 'protein', 'binding', 'happens', 'with', 'Arbekacin'], ['Cholic', 'Acid', 'addresses', 'which', 'target', 'to', 'achieve', 'the', 'desired', 'effect'], ['which', 'cellular', 'location', 'we', 'can', 'find', 'enzyme', 'Cytochrome', 'P450', '2B7', 'isoform'], ['what', 'is', 'the', 'agent', 'category', 'for', 'Cefamandole'], ['explain', 'pharmacology', 'details', 'of', 'ACA', '125'], ['Acetylsalicylic', 'acid', 'is', 'biotransformed', 'using', 'which', 'process'], ['provide', 'the', 'percentage', 'of', 'Mupirocin', 'bound', 'to', 'proteins'], ['ado', '-', 'trastuzumab', 'emtansine', 'can', 'affect', 'which', 'life', 'forms'], ['Cloxacillin', 'is', 'exisit', 'in', 'which', 'structure'], ['what', 'are', 'food', 'advices', 'for', 'adminstration', 'of', 'Isosorbide', 'Mononitrate'], ['what', 'is', 'the', 'name', 'of', 'kingdom', 'compound', 'for', 'Isosorbide', 'Dinitrate'], ['how', 'Radium', 'Ra', '223', 'Dichloride', 'is', 'transformed', 'during', 'metabolism'], ['list', 'the', 'categories', 'into', 'which', 'Thiamine', 'can', 'be', 'classified']]\n",
            "Predicted Labels\n",
            "[['B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'I-E'], ['O', 'B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'B-E', 'O', 'O'], ['O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'B-E', 'B-E', 'B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'I-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O']]\n",
            "Actual Labels\n",
            "[['B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'O', 'O'], ['O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'B-E', 'B-E', 'B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'I-E'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['B-E', 'B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'I-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O']]\n",
            "Predicted Entities\n",
            "['Nitroglycerin', 'Phenmetrazine', 'Nepafenac', 'Hydrochlorothiazide', 'Zafirlukast', 'Theophylline', 'enzme Aldehyde oxidase', 'Vitamin D3 receptoroelectric', 'Glutethimide', 'L-Cysteine', 'Flurandrenolide', 'Dipivefrin', 'Flavohemoprotein', 'Preotact', 'MGCD-0103', 'enzme Cytochrome P450 2B7 isoform', 'Deferasirox', 'Aldehyde', 'Ibuprofen', 'Transthyretin', 'Cefmetazole', 'Glycine', 'Hydromorphone', 'Fluticasone Propionate', 'Ketoconazole', 'Phenylephrine', 'Estriol', 'Acetyl-CoA carboxylase 2', 'Fusidic Acid', 'Simvastatin', 'Peginterferon alfa-2b', 'Loratadine', 'NBI-6024', 'Cephalexin', 'AZD6140', 'Vitamin D-binding protein', 'Warfarinstibuted', 'Pegaptanib', 'Human Serum Albumin', 'MEM 1414', 'Delorazepam', 'Menthol', 'Cytochrome P450 2B7 isoform', 'Chlorphenamine', 'Diclofenac', 'Venlafaxine', 'Hydrolase', 'Gold bond medicated lotion', 'Beta-lactamase TEM', 'Isradipine', '33 kDa chaperonin', 'Ammonia channel', 'Moxifloxacin', 'Aldehyde oxidase', 'Ziprasidoneministrated', 'Cytochrome P450 2B7 isoform', 'Thyroxine-binding globulin', 'Lepirudin', 'Clobetasol propionate', 'Ioflupane I 123', 'Subtilisin Savinase', 'Oxiconazole', 'Vitamin D-binding', 'Timolol', 'Dobutamine', 'Thyroxine-binding globulin', 'Pergolideharmacology', 'Peptide deformylase', 'Icosapent', 'Dihomo-γ-linolenic acidvilable', 'Lactotransferrin', 'Lindane', 'Denileukin diftitox', 'Metronidazole', 'Axitinib', 'Zidovudine', 'Arylamine N-acetyltransferase', 'Bronchyl SYR', 'Aclidinium', '19-norandrostenedione', 'Adefovir Dipivoxil', 'Glutathione peroxidase 1', 'Chlorprothixene', 'Tauroursodeoxycholic acid', 'Lin o gel', 'Ibandronate', 'Flavopiridol', 'Magnesium oxide', 'Aldehyde oxidasei', 'DNA polymerase I', 'Codeine', 'Clopidogrel', 'Ketobemidone', 'Gadofosveset trisodium', 'L-Phenylalanine', 'Sodium bicarbonate', 'Isosorbide Dinitrate', 'Glucagon recombinant', 'Cinoxacin', 'Potassium Chloride', 'Cefuroxime', 'Arbekacin', 'Cholic Acid', 'Cytochrome P450 2B7 isoform', 'Cefamandole', 'ACA 125', 'Acetylsalicylic acidtransformed', 'Mupirocin', '-trastuzumab emtansine', 'Cloxacillinisit', 'Isosorbide Mononitrate', 'Isosorbide Dinitrate', 'Radium Ra 223 Dichloride', 'Thiamine']\n",
            "Actual Entities\n",
            "['Nitroglycerin', 'Phenmetrazine', 'Nepafenac', 'Hydrochlorothiazide', 'Zafirlukast', 'Theophylline', 'Aldehyde oxidase', 'Vitamin D3 receptoroelectric', 'Glutethimide', 'L-Cysteine', 'Flurandrenolide', 'Dipivefrin', 'Flavohemoprotein', 'Preotact', 'MGCD-0103', 'Cytochrome P450 2B7 isoform', 'Deferasirox', 'enzyme Aldehyde', 'Ibuprofen', 'Transthyretin', 'Cefmetazole', 'Glycine', 'Hydromorphone', 'Fluticasone Propionate', 'Ketoconazole', 'Phenylephrine', 'Estriol', 'Acetyl-CoA carboxylase 2', 'Fusidic Acid', 'Simvastatin', 'Peginterferon alfa-2b', 'Loratadine', 'NBI-6024', 'Cephalexin', 'AZD6140', 'Vitamin D-binding protein', 'Warfarinstibuted', 'Pegaptanib', 'Human Serum Albumin', 'MEM 1414', 'Delorazepam', 'Menthol', 'Cytochrome P450 2B7 isoform', 'Chlorphenamine', 'Diclofenac', 'Venlafaxine', 'Hydrolase', 'Gold bond medicated lotion', 'Beta-lactamase TEM', 'Isradipine', '33 kDa chaperonin', 'Ammonia channel', 'Moxifloxacin', 'Aldehyde oxidase', 'Ziprasidoneministrated', 'Cytochrome P450 2B7 isoform', 'Thyroxine-binding globulin', 'Lepirudin', 'Clobetasol propionate', 'Ioflupane I 123', 'Subtilisin Savinase', 'Oxiconazole', 'for Vitamin D-binding', 'Timolol', 'Dobutamine', 'Thyroxine-binding globulin', 'Pergolideharmacology', 'Peptide deformylase', 'Icosapent', 'Dihomo-γ-linolenic acidvilable', 'Lactotransferrin', 'Lindane', 'Denileukin diftitox', 'Metronidazole', 'Axitinib', 'Zidovudine', 'Arylamine N-acetyltransferase', 'Bronchyl SYR', 'Aclidinium', '19-norandrostenedione', 'Adefovir Dipivoxil', 'Glutathione peroxidase 1', 'Chlorprothixene', 'Tauroursodeoxycholic acid', 'Lin o gel', 'Ibandronate', 'Flavopiridol', 'Magnesium oxide', 'Aldehyde oxidasei', 'DNA polymerase I', 'Codeine', 'Clopidogrel', 'Ketobemidone', 'Gadofosveset trisodium', 'L-Phenylalanine', 'Sodium bicarbonate', 'Isosorbide Dinitrate', 'Glucagon recombinant', 'Cinoxacin', 'Potassium Chloride', 'Cefuroxime', 'Arbekacin', 'Cholic Acid', 'Cytochrome P450 2B7 isoform', 'Cefamandole', 'ACA 125', 'Acetylsalicylic acidtransformed', 'Mupirocin', 'ado-trastuzumab emtansine', 'Cloxacillinisit', 'Isosorbide Mononitrate', 'Isosorbide Dinitrate', 'Radium Ra 223 Dichloride', 'Thiamine']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwuyWgqZT310",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr9JrWZuTb6i",
        "colab_type": "text"
      },
      "source": [
        "**References**\n",
        "\n",
        "Followed Examples from\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/\n",
        "\n",
        "https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "\n",
        "http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
        "\n",
        "https://www.kaggle.com/nkaenzig/bert-tensorflow-2-huggingface-transformers\n",
        "\n",
        "https://colab.research.google.com/drive/1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU#scrollTo=tBa6vRHknSkv\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}