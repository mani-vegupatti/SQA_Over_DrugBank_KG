{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_Model02_BERT_Frozen_V2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGnFcfmxhFvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8dea77f7-00fd-4176-a344-cc627538ba62"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNwFtQLfh2oW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "99b48d8e-ccd7-4ab6-bd4a-75310a914b9e"
      },
      "source": [
        "# set google drive for files\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/drive/My Drive/Colab Notebooks/temp/b08d5871a151.json\"\n",
        "!echo $GOOGLE_APPLICATION_CREDENTIALS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/temp/b08d5871a151.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndGmJx-nh3Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set path for files\n",
        "path = \"/content/drive/My Drive/thesis_dataset/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2My7CPyhhFGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "22738535-948e-45ec-a72f-9b678f31dd4c"
      },
      "source": [
        "# install required packages\n",
        "!pip install transformers\n",
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.2.4->seqeval) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cGAEKO1h6-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0bfdac1-ee4a-4437-b7ad-1ac2fa7cfdeb"
      },
      "source": [
        "# import required packages/modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "\n",
        "print(f'Torch: {torch.__version__}, Transformers: {transformers.__version__}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch: 1.6.0+cu101, Transformers: 3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41mAaRxRFgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define constants\n",
        "MAX_TOKENS = 64\n",
        "BATCH_SIZE = 32\n",
        "NR_EPOCHES = 20\n",
        "MAX_NORM = 1.0\n",
        "BERT_PRETRAIN_MODEL_NAME = \"bert-base-cased\"\n",
        "NR_WARM_STEPS = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGxwV6fJh9p5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "98881c07-0c06-490c-ebfb-5d9ff8916558"
      },
      "source": [
        "# read data from excel\n",
        "df = pd.read_excel(path+\"All_Questions_V1.xlsx\",'data', encoding='utf-8') \n",
        "df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SlNo</th>\n",
              "      <th>Question</th>\n",
              "      <th>Relation</th>\n",
              "      <th>NER_Tag</th>\n",
              "      <th>Q_Len</th>\n",
              "      <th>T_Len</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Subject_URI</th>\n",
              "      <th>Relation_URI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>what are the brand names of Metipranolol</td>\n",
              "      <td>brand</td>\n",
              "      <td>O O O O O O B-E</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>Metipranolol</td>\n",
              "      <td>http://bio2rdf.org/drugbank:DB01214</td>\n",
              "      <td>http://bio2rdf.org/drugbank_vocabulary:brand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SlNo  ...                                  Relation_URI\n",
              "0     1  ...  http://bio2rdf.org/drugbank_vocabulary:brand\n",
              "\n",
              "[1 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZtLr2gOqy_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b90a259-3e5a-4167-bab6-096af1920cf8"
      },
      "source": [
        "# split the full dataset into train, valid and test dataset\n",
        "rest, test = train_test_split(df, test_size=0.2, random_state=0, \n",
        "                               stratify=df['Relation'])\n",
        "train, valid = train_test_split(rest, test_size=0.1, random_state=0, \n",
        "                               stratify=rest['Relation'])\n",
        "print(f'Train:{len(train)}, Test: {len(test)}, Validation: {len(valid)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train:406, Test: 114, Validation: 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmFJoiioRPuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c0d98b44-9a71-4d32-c418-d6dd60506306"
      },
      "source": [
        "# make the processing device as GPU or CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMpcU7SYQsrB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "867a997e-76da-4f49-af18-844f00696add"
      },
      "source": [
        "# create dictionary of NER_TAGs\n",
        "tag_ids = ['O', 'B-E', 'I-E', 'PAD']\n",
        "tag_dict = {t: i for i, t in enumerate(tag_ids)}\n",
        "num_ner_tags = len(tag_dict)\n",
        "print(num_ner_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCbD1wVlQ8IW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create instance of tokenzier from BERT pretrained model\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_PRETRAIN_MODEL_NAME, do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_fcWYWDRhb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# process the question phrase to return tokens list\n",
        "# process the NER_TAGs to match wordpieces of tokenizer\n",
        "def process_tokens_labels(sent, labels):\n",
        "    tokens_list = []\n",
        "    labels_list = []\n",
        "    for word, label in zip(sent, labels):\n",
        "        # process tokens\n",
        "        tokens = tokenizer.tokenize(word)\n",
        "        tokens_list.extend(tokens)\n",
        "        # process labels\n",
        "        num_wordpieces = len(tokens)\n",
        "        labels_list.extend([label] * num_wordpieces)\n",
        "    return tokens_list, labels_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW2WCqa8QDLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# process the question phrase to return Torch tensors\n",
        "# process the NER_TAGs to retun Torch tensors\n",
        "def process_data(df_data):\n",
        "  # process input data\n",
        "  words_list = [[word for word in sentence.split()] for sentence in df_data['Question'].values]\n",
        "  print(words_list[0])\n",
        "  labels_list = [[tag for tag in tag_value.split()] for tag_value in df_data['NER_Tag'].values]\n",
        "  print(labels_list[0])\n",
        "  # gets inputs_ids and attention masks\n",
        "  tokens_with_labels = [process_tokens_labels(sentence, labels)\n",
        "                                for sentence, labels in zip(words_list, labels_list)]\n",
        "  tokens_list = [token_with_label[0] for token_with_label in tokens_with_labels]\n",
        "  input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(token) for token in tokens_list], maxlen=MAX_TOKENS,  \n",
        "                            truncating=\"post\", padding=\"post\", value=0.0, dtype=\"long\")\n",
        "  attn_masks = [[float(id != 0.0) for id in input_id] for input_id in input_ids]\n",
        "  # process labels and convert to numbers\n",
        "  new_labels_list = [token_with_label[1] for token_with_label in tokens_with_labels]\n",
        "  target_labels = pad_sequences([[tag_dict[lab] for lab in label] for label in new_labels_list], maxlen=MAX_TOKENS, \n",
        "                       truncating=\"post\", padding=\"post\", value=tag_dict[\"PAD\"], dtype=\"long\",)\n",
        "  \n",
        "  return torch.tensor(input_ids), torch.tensor(attn_masks), torch.tensor(target_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EegBf0sXQAS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ef5f6645-e45b-417c-a06e-b23844718c0c"
      },
      "source": [
        "# process question phrases and NER_TAGs to get Torch tensors\n",
        "train_input_ids, train_attn_masks, train_ner_tags  = process_data(train)\n",
        "valid_input_ids, valid_attn_masks, valid_ner_tags  = process_data(valid)\n",
        "test_input_ids, test_attn_masks, test_ner_tags  = process_data(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['which', 'life', 'forms', 'are', 'impacted', 'by', 'Marimastat']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'B-E']\n",
            "['what', 'is', 'the', 'volume', 'of', 'distribution', 'for', 'Coagulation', 'factor', 'VIIa']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E']\n",
            "['Nitroglycerin', 'is', 'patented', 'under', 'which', 'number']\n",
            "['B-E', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J5YBCNFlXtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Process question phrases and NER_Tags to get datloader\n",
        "train_dataset = TensorDataset(train_input_ids, train_attn_masks, train_ner_tags)\n",
        "train_random_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_random_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_dataset = TensorDataset(valid_input_ids, valid_attn_masks, valid_ner_tags)\n",
        "valid_random_sampler = SequentialSampler(valid_dataset)\n",
        "valid_dataloader = DataLoader(valid_dataset, sampler=valid_random_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_attn_masks, test_ner_tags)\n",
        "test_random_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_random_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50FfBHF8lgJu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "9eaefcf2-317e-4f4b-9fa1-ae39f57f7fa2"
      },
      "source": [
        "# create model from pretrained BERT model\n",
        "# send the model parameters to default device\n",
        "model = BertForTokenClassification.from_pretrained( BERT_PRETRAIN_MODEL_NAME , num_labels=num_ner_tags, output_attentions = False,output_hidden_states = False)\n",
        "model.cuda();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W6G7QxclmVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretrained BERT base layers are frozen\n",
        "# determine parameters of classifier layers only and create optimizer\n",
        "parameters = list(model.classifier.named_parameters())\n",
        "optimizer_parameters = [{\"params\": [parameter for num, parameter in parameters]}]\n",
        "optimizer = AdamW(optimizer_parameters)\n",
        "\n",
        "# Determine training steps and create scheduler\n",
        "train_steps = len(train_dataloader) * NR_EPOCHES\n",
        "scheduler = get_linear_schedule_with_warmup( optimizer, NR_WARM_STEPS, train_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM8zmYVAJh6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to update the parameters during training\n",
        "def model_training(train_dataloader):\n",
        "    # model in training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    # train the model and update parameters of classifier layer\n",
        "    for train_instance in train_dataloader:\n",
        "        train_data_row = tuple(row.to(device) for row in train_instance)\n",
        "        input_ids, attn_mask, labels = train_data_row\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attn_mask, labels=labels)\n",
        "        instance_loss = outputs[0]\n",
        "        instance_loss.backward()\n",
        "        train_loss += instance_loss.item()\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_NORM)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    return train_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2m-SnYHJitA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to calculate evaluation metrics during validation\n",
        "def model_validation(valid_dataloader):\n",
        "    # model in evaluation mode\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    pred_labels = [] \n",
        "    act_labels = []\n",
        "    tokens = []\n",
        " \n",
        "    # find predicted NER_TAGs and retrieve actual NER_TAGs from tensor\n",
        "    for valid_instance in valid_dataloader:\n",
        "        valid_data = tuple(row.to(device) for row in valid_instance)\n",
        "        input_ids, attn_mask, labels = valid_data\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attn_mask, labels=labels)\n",
        "        instance_loss = outputs[0]\n",
        "        valid_loss += instance_loss.item()\n",
        "\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        pred_labels.extend([list(p_labels) for p_labels in np.argmax(logits, axis=2)])\n",
        "        act_label = labels.to('cpu').numpy()\n",
        "        act_labels.extend(act_label)\n",
        "\n",
        "        for input_id in input_ids:\n",
        "          tokens.extend([tokenizer.convert_ids_to_tokens(input_id.to('cpu').numpy())])\n",
        "\n",
        "    return valid_loss, pred_labels, act_labels, tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzVMd0w6Jlys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ef6b80f-259d-4f24-e2b9-22801569d5ef"
      },
      "source": [
        "# train the model for required epoches \n",
        "for epoch_num in tqdm(range(NR_EPOCHES), desc=\"Training Progress\"):\n",
        "    num_train_samples = len(train_dataloader)\n",
        "    num_valid_samples = len(valid_dataloader)\n",
        "\n",
        "    train_loss = model_training(train_dataloader)\n",
        "    valid_loss, pred_labels, act_labels, _ = model_validation(valid_dataloader)\n",
        "\n",
        "    # calculate and print training loss\n",
        "    training_loss = train_loss / num_train_samples\n",
        "    print()\n",
        "    print(f'Training loss: {training_loss}')\n",
        "\n",
        "    # calculate and print validation loss, accuracy and F-Score\n",
        "    validation_loss = valid_loss / num_valid_samples\n",
        "    print(f'Validation loss: {validation_loss}')\n",
        "    pred_ner_tags = [tag_ids[pred] for pred_label, act_label in zip(pred_labels, act_labels)\n",
        "                                 for pred, act in zip(pred_label, act_label) if tag_ids[act] != \"PAD\"]\n",
        "    act_ner_tags = [tag_ids[act] for act_label in act_labels\n",
        "                                  for act in act_label if tag_ids[act] != \"PAD\"]\n",
        "    print(f'Validation Accuracy: {accuracy_score(pred_ner_tags, act_ner_tags)}')\n",
        "    print(f'Validation F-Score: {f1_score(pred_ner_tags, act_ner_tags)}')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Progress:   5%|▌         | 1/20 [00:04<01:24,  4.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 1.0307615995407104\n",
            "Validation loss: 0.8535871803760529\n",
            "Validation Accuracy: 0.7495741056218058\n",
            "Validation F-Score: 0.6420323325635104\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  10%|█         | 2/20 [00:08<01:20,  4.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.6852038961190444\n",
            "Validation loss: 0.633705198764801\n",
            "Validation Accuracy: 0.807495741056218\n",
            "Validation F-Score: 0.7044917257683215\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  15%|█▌        | 3/20 [00:13<01:16,  4.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.5495945880046258\n",
            "Validation loss: 0.558198869228363\n",
            "Validation Accuracy: 0.8109028960817717\n",
            "Validation F-Score: 0.7152941176470589\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  20%|██        | 4/20 [00:18<01:12,  4.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.4828859567642212\n",
            "Validation loss: 0.5171805173158646\n",
            "Validation Accuracy: 0.8296422487223168\n",
            "Validation F-Score: 0.7373493975903614\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  25%|██▌       | 5/20 [00:22<01:08,  4.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.433902953679745\n",
            "Validation loss: 0.48581562936306\n",
            "Validation Accuracy: 0.8364565587734242\n",
            "Validation F-Score: 0.7311320754716981\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  30%|███       | 6/20 [00:27<01:04,  4.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.4034388867708353\n",
            "Validation loss: 0.4618489146232605\n",
            "Validation Accuracy: 0.8466780238500852\n",
            "Validation F-Score: 0.7398568019093079\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  35%|███▌      | 7/20 [00:32<01:00,  4.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.38791420597296494\n",
            "Validation loss: 0.44724270701408386\n",
            "Validation Accuracy: 0.8534923339011925\n",
            "Validation F-Score: 0.7600950118764844\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  40%|████      | 8/20 [00:36<00:55,  4.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.3777966774426974\n",
            "Validation loss: 0.4307478368282318\n",
            "Validation Accuracy: 0.8551959114139693\n",
            "Validation F-Score: 0.7541766109785204\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  45%|████▌     | 9/20 [00:41<00:50,  4.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.3620133835535783\n",
            "Validation loss: 0.4191795438528061\n",
            "Validation Accuracy: 0.858603066439523\n",
            "Validation F-Score: 0.7589498806682576\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  50%|█████     | 10/20 [00:45<00:45,  4.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.3502081563839546\n",
            "Validation loss: 0.4070238620042801\n",
            "Validation Accuracy: 0.858603066439523\n",
            "Validation F-Score: 0.7589498806682576\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  55%|█████▌    | 11/20 [00:50<00:40,  4.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.3432680368423462\n",
            "Validation loss: 0.3988337069749832\n",
            "Validation Accuracy: 0.8620102214650767\n",
            "Validation F-Score: 0.7589498806682576\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  60%|██████    | 12/20 [00:54<00:36,  4.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.33296764355439407\n",
            "Validation loss: 0.39083176851272583\n",
            "Validation Accuracy: 0.8637137989778535\n",
            "Validation F-Score: 0.7559808612440191\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  65%|██████▌   | 13/20 [00:59<00:31,  4.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.3288194640324666\n",
            "Validation loss: 0.3877408057451248\n",
            "Validation Accuracy: 0.8620102214650767\n",
            "Validation F-Score: 0.7541766109785204\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  70%|███████   | 14/20 [01:03<00:26,  4.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.3212138322683481\n",
            "Validation loss: 0.385496586561203\n",
            "Validation Accuracy: 0.8620102214650767\n",
            "Validation F-Score: 0.7541766109785204\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  75%|███████▌  | 15/20 [01:08<00:22,  4.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.3249700149664512\n",
            "Validation loss: 0.38112545013427734\n",
            "Validation Accuracy: 0.8671209540034072\n",
            "Validation F-Score: 0.762589928057554\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  80%|████████  | 16/20 [01:12<00:17,  4.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.32153326960710377\n",
            "Validation loss: 0.3799140453338623\n",
            "Validation Accuracy: 0.8671209540034072\n",
            "Validation F-Score: 0.762589928057554\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  85%|████████▌ | 17/20 [01:16<00:13,  4.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.30920888712772954\n",
            "Validation loss: 0.37929438054561615\n",
            "Validation Accuracy: 0.8654173764906303\n",
            "Validation F-Score: 0.7577937649880094\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  90%|█████████ | 18/20 [01:21<00:08,  4.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.3132654382632329\n",
            "Validation loss: 0.37740761041641235\n",
            "Validation Accuracy: 0.8654173764906303\n",
            "Validation F-Score: 0.7577937649880094\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTraining Progress:  95%|█████████▌| 19/20 [01:25<00:04,  4.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.30267587074866664\n",
            "Validation loss: 0.37677186727523804\n",
            "Validation Accuracy: 0.8654173764906303\n",
            "Validation F-Score: 0.7577937649880094\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 20/20 [01:30<00:00,  4.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training loss: 0.3160336625117522\n",
            "Validation loss: 0.37680476903915405\n",
            "Validation Accuracy: 0.8654173764906303\n",
            "Validation F-Score: 0.7577937649880094\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxGtTEqtJpqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to evaluate any given dataset\n",
        "def evaluate_model(dataloader):\n",
        "    num_valid_samples = len(dataloader)\n",
        "    calc_loss, pred_labels, act_labels, tokens = model_validation(dataloader)\n",
        "\n",
        "    # calculate and print validation loss, accuracy and F-Score\n",
        "    final_loss = calc_loss / num_valid_samples\n",
        "    print(f'Loss: {final_loss}')\n",
        "    pred_ner_tags = [tag_ids[pred] for pred_label, act_label in zip(pred_labels, act_labels)\n",
        "                                 for pred, act in zip(pred_label, act_label) if tag_ids[act] != \"PAD\"]\n",
        "    act_ner_tags = [tag_ids[act] for act_label in act_labels\n",
        "                                  for act in act_label if tag_ids[act] != \"PAD\"]\n",
        "    print(f'Accuracy: {accuracy_score(pred_ner_tags, act_ner_tags)}')\n",
        "    print(f'F-Score: {f1_score(pred_ner_tags, act_ner_tags)}')\n",
        "    print(classification_report(pred_ner_tags, act_ner_tags))    \n",
        "    print()\n",
        "\n",
        "    # reconstruct tokens, lables and entities\n",
        "    # print actual and predicted for visual comparision\n",
        "    p_labels_list, a_labels_list, tokens_list, a_entities_list, p_entities_list = [], [], [], [], []\n",
        "    for token, prd_label, act_label in zip(tokens, pred_labels, act_labels ):\n",
        "      new_p_labels, new_a_labels, new_tokens = [], [], []\n",
        "      a_entity, p_entity = \"\", \"\"\n",
        "      a_done_flag, p_done_flag = False, False\n",
        "      a_inside_flag, p_inside_flag = False, False\n",
        "      a_prev_tag, p_prev_tag = 'O', 'O'\n",
        "      for token, label_idx, t_label_idx in zip(token, prd_label, act_label):\n",
        "        if t_label_idx != 3:\n",
        "          if token.startswith(\"##\"):\n",
        "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "\n",
        "            if not(a_done_flag) and a_inside_flag:\n",
        "              a_entity += token[2:]\n",
        "\n",
        "            if not(p_done_flag) and p_inside_flag:\n",
        "              p_entity += token[2:]\n",
        "          else:\n",
        "            new_p_labels.append(tag_ids[label_idx])\n",
        "            new_a_labels.append(tag_ids[t_label_idx])\n",
        "            new_tokens.append(token)\n",
        "\n",
        "            a_curnt_tag = tag_ids[t_label_idx]\n",
        "            if not(a_done_flag):\n",
        "              if a_curnt_tag in ['B-E', 'I-E']:\n",
        "                if token not in [\"'\", \"s\"]:\n",
        "                  if token == \"-\" or a_entity[-1:] == \"-\":\n",
        "                    a_entity = a_entity+ token\n",
        "                  elif a_entity == \"\":\n",
        "                    a_entity = token\n",
        "                  else:\n",
        "                    a_entity = a_entity+ \" \" +token\n",
        "                  a_inside_flag = True\n",
        "              else:\n",
        "                if a_prev_tag in ['B-E', 'I-E']:\n",
        "                  a_done_flag = True\n",
        "            \n",
        "            p_curnt_tag = tag_ids[label_idx]\n",
        "            if not(p_done_flag):\n",
        "              if p_curnt_tag in ['B-E', 'I-E']:\n",
        "                if token not in [\"'\", \"s\"]:\n",
        "                  if token == \"-\" or p_entity[-1:] == \"-\":\n",
        "                    p_entity = p_entity+ token\n",
        "                  elif p_entity == \"\":\n",
        "                    p_entity = token\n",
        "                  else:\n",
        "                    p_entity = p_entity+ \" \" +token\n",
        "                  p_inside_flag = True\n",
        "              else:\n",
        "                if p_prev_tag in ['B-E', 'I-E']:\n",
        "                  p_done_flag = True\n",
        "        \n",
        "      tokens_list.append(new_tokens) \n",
        "      p_labels_list.append(new_p_labels)    \n",
        "      a_labels_list.append(new_a_labels)\n",
        "      a_entities_list.append(a_entity)\n",
        "      p_entities_list.append(p_entity)\n",
        "\n",
        "    print(\"Tokens List\")\n",
        "    print(tokens_list)\n",
        "    print(\"Predicted Labels\")\n",
        "    print(p_labels_list)\n",
        "    print(\"Actual Labels\")\n",
        "    print(a_labels_list)\n",
        "    print(\"Predicted Entities\")\n",
        "    print(p_entities_list)\n",
        "    print(\"Actual Entities\")\n",
        "    print(a_entities_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwwecQ9fYdQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "e79af0c0-d777-42d8-fd44-f459163db496"
      },
      "source": [
        "print(f'                 Validation Dataset Results                  ')\n",
        "print(\"--------------------------------------------------------------\")\n",
        "evaluate_model(valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 Validation Dataset Results                  \n",
            "--------------------------------------------------------------\n",
            "Loss: 0.37680476903915405\n",
            "Accuracy: 0.8654173764906303\n",
            "F-Score: 0.7577937649880094\n",
            "           precision    recall  f1-score   support\n",
            "\n",
            "        E       0.83      0.70      0.76       226\n",
            "\n",
            "micro avg       0.83      0.70      0.76       226\n",
            "macro avg       0.83      0.70      0.76       226\n",
            "\n",
            "\n",
            "Tokens List\n",
            "[['what', 'is', 'the', 'volume', 'of', 'distribution', 'for', 'Coagulation', 'factor', 'VIIa'], ['what', 'is', 'exact', 'the', 'position', 'of', 'Ferritin', 'heavy', 'chain', 'on', 'a', 'chromosome'], ['how', 'the', 'interaction', 'of', 'Dronedarone', 'affects', 'other', 'drugs', \"'\", 's', 'actions'], ['which', 'is', 'the', 'kingdom', 'grouping', 'of', 'the', 'drug', 'Alpha', '-', 'Linolenic', 'Acid'], ['provide', 'the', 'general', 'activities', 'carried', 'out', 'by', 'Protein', 'S100', '-', 'A1'], ['Leukotriene', 'C4', 'synthase', 'is', 'encoded', 'by', 'which', 'gene'], ['how', 'Nandrolone', 'decanoate', 'is', 'metabolised'], ['provide', 'the', 'patent', 'number', 'filed', 'for', 'Mirabegron'], ['what', 'are', 'the', 'foods', 'to', 'consume', 'and', 'avoid', 'when', 'taking', 'Aminoglutethimide'], ['Quinapril', 'is', 'content', 'of', 'which', 'mixtures'], ['Acetaminophen', 'present', 'in', 'which', 'mixture'], ['provide', 'the', 'name', 'of', 'packaging', 'company', 'for', 'Diltiazem'], ['what', 'are', 'the', 'other', 'well', 'known', 'names', 'of', 'Lincomycin'], ['provide', 'the', 'active', 'drug', 'removal', 'rate', 'for', 'Rocuronium'], ['what', 'are', 'the', 'possible', 'organisms', 'impacted', 'by', 'Insulin', 'Glargine'], ['how', 'much', 'of', 'Allopurinol', 'is', 'removed', 'during', 'elimination', 'process'], ['what', 'are', 'the', 'current', 'status', 'groups', 'of', 'Estrone'], ['what', 'is', 'the', 'value', 'of', 'locus', 'for', 'Egl', 'nine', 'homolog', '1'], ['Nitrofurantoin', 'falls', 'under', 'which', 'category'], ['how', 'much', 'volume', 'of', 'Clomipramine', 'is', 'distributed', 'after', 'administration', 'in', 'body', 'fluids'], ['which', 'prescription', 'product', 'forms', 'are', 'available', 'for', 'Nizatidine'], ['which', 'molecule', 'and', 'function', 'is', 'targeted', 'by', 'Pyridoxal', 'Phosphate'], ['provide', 'the', 'actual', 'pi', 'value', 'of', 'Endo', '-', 'N', '-', 'acetylneuraminidase'], ['Photoactive', 'yellow', 'protein', 'is', 'involved', 'in', 'which', 'basic', 'functions'], ['Cycrimine', 'is', 'grouped', 'into', 'which', 'kingdom', 'of', 'compounds'], ['provide', 'molecular', 'weight', 'of', 'enzyme', 'Aldehyde', 'oxidase'], ['N', '-', 'Cholylglycine', \"'\", 's', 'efflux', 'is', 'carried', 'out', 'by', 'which', 'transporter'], ['provide', 'protein', 'binding', 'value', 'of', 'Dofetilide'], ['list', 'the', 'all', 'known', 'side', 'effects', 'on', 'using', 'Levetiracetam'], ['Penbutolol', 'is', 'used', 'in', 'which', 'end', 'products'], ['what', 'are', 'the', 'details', 'in', 'the', 'pharmacology', 'report', 'of', 'Botulinum', 'Toxin', 'Type', 'A'], ['name', 'the', 'brands', 'for', 'PA', 'mAb'], ['who', 'produces', 'Penicillin', 'V'], ['list', 'the', 'specific', 'functions', 'carried', 'out', 'by', 'Exotoxin', 'A'], ['Growth', 'hormone', 'receptor', 'is', 'available', 'in', 'which', 'organisms'], ['explain', 'the', 'working', 'of', 'Stanozolol', 'for', 'inducing', 'the', 'required', 'results'], ['what', 'is', 'the', 'unit', 'dose', 'form', 'of', 'Pentosan', 'Polysulfate'], ['list', 'the', 'salts', 'of', 'Grepafloxacin'], ['what', 'are', 'the', 'indications', 'treated', 'using', 'Sibutramine'], ['at', 'which', 'location', 'of', 'a', 'cell', 'we', 'can', 'find', 'concentrations', 'of', 'Protein', 'UshA'], ['how', 'much', 'Pseudoephedrine', 'binds', 'to', 'protein'], ['what', 'is', 'Peginterferon', 'alfa', '-', '2a', \"'\", 's', 'half', 'life', 'value'], ['provide', 'Sodium', 'Tetradecyl', 'Sulfate', \"'\", 's', 'structure', 'name'], ['recombinant', 'human', 'GM', '-', 'CSF', 'is', 'helpful', 'in', 'treating', 'which', 'signs', 'and', 'symptoms'], ['which', 'gene', 'can', 'be', 'helpful', 'to', 'produce', 'Protein', 'Nef'], ['what', 'are', 'Marvelon', '21', 'Tab', \"'\", 's', 'ingredients']]\n",
            "Predicted Labels\n",
            "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O', 'O', 'O'], ['O', 'B-E', 'I-E', 'O', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'B-E'], ['B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E'], ['B-E', 'O', 'B-E', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'B-E', 'B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'B-E', 'B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'O', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O']]\n",
            "Actual Labels\n",
            "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-E', 'I-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'B-E', 'B-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['B-E', 'B-E', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O', 'O', 'O'], ['O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O', 'O'], ['B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E', 'O']]\n",
            "Predicted Entities\n",
            "['Coagulation VIIa', 'Ferritin heavy', 'Dronedarone', 'drug Alpha-Linolenic Acid', 'Protein S100-A1', 'Leukotriene C4ynthase is encoded', 'Nandrolone decanoate metabolised', 'Mirabegron', 'Aminoglutethimide', '', '', 'Diltiazem', 'Lincomycin', 'Rocuronium', 'Insulin Glargine', 'Allopurinol', 'Estrone', 'Egl nine homolog 1', 'Nitrofurantoin', 'Clomipramine', 'Nizatidine', 'Pyridoxal Phosphate', 'Endo-N-acetylneuraminidase', '', 'Cycrimine', 'Aldehyde oxidase', '-Cholylglycineffluxer', 'Dofetilide', 'Levetiracetam', '', 'Botulinum Toxin Type A', 'PA mAb', 'who Penicillin', 'Exotoxin A', 'Growth', 'Stanozololducing', 'Pentosan Polysulfate', 'Grepafloxacin', 'Sibutramine', 'Protein UshA', 'Pseudoephedrine', 'Peginterferon alfa-2a', 'Sodium Tetradecyl Sulfate', 'human GM-CSF', 'Protein Nef', 'Marvelon 21 Tab']\n",
            "Actual Entities\n",
            "['Coagulation factor VIIa', 'Ferritin heavy chain', 'Dronedarone', 'Alpha-Linolenic Acid', 'Protein S100-A1', 'Leukotriene C4ynthase', 'Nandrolone decanoatebolised', 'Mirabegron', 'Aminoglutethimide', 'Quinaprils', 'Acetaminophen', 'Diltiazem', 'Lincomycin', 'Rocuronium', 'Insulin Glargine', 'Allopurinol', 'Estrone', 'Egl nine homolog 1', 'Nitrofurantoin', 'Clomipramine', 'Nizatidine', 'Pyridoxal Phosphate', 'Endo-N-acetylneuraminidase', 'Photoactive yellow protein', 'Cycrimine', 'Aldehyde oxidase', 'N-Cholylglycineffluxer', 'Dofetilide', 'Levetiracetam', 'Penbutolol', 'Botulinum Toxin Type A', 'PA mAb', 'Penicillin V', 'Exotoxin A', 'Growth hormone receptor', 'Stanozololducing', 'Pentosan Polysulfate', 'Grepafloxacin', 'Sibutramine', 'Protein UshA', 'Pseudoephedrine', 'Peginterferon alfa-2a', 'Sodium Tetradecyl Sulfate', 'recombinant human GM-CSF', 'Protein Nef', 'Marvelon 21 Tab']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6qTP_g8Z0p_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "9467d102-9ad6-4220-a4dd-0070fd445b4f"
      },
      "source": [
        "print(f'                 Test Dataset Results                  ')\n",
        "print(\"--------------------------------------------------------------\")\n",
        "evaluate_model(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 Test Dataset Results                  \n",
            "--------------------------------------------------------------\n",
            "Loss: 0.2970920577645302\n",
            "Accuracy: 0.9046666666666666\n",
            "F-Score: 0.8103946102021174\n",
            "           precision    recall  f1-score   support\n",
            "\n",
            "        E       0.86      0.77      0.81       549\n",
            "\n",
            "micro avg       0.86      0.77      0.81       549\n",
            "macro avg       0.86      0.77      0.81       549\n",
            "\n",
            "\n",
            "Tokens List\n",
            "[['Nitroglycerin', 'is', 'patented', 'under', 'which', 'number'], ['which', 'companies', 'manufacture', 'Phenmetrazine'], ['list', 'all', 'synonyms', 'of', 'Nepafenac'], ['list', 'the', 'mixtures', 'that', 'contains', 'Hydrochlorothiazide'], ['which', 'is', 'the', 'transporter', 'for', 'Zafirlukast'], ['what', 'are', 'the', 'overdose', 'impacts', 'of', 'Theophylline'], ['what', 'is', 'the', 'general', 'function', 'of', 'enzme', 'Aldehyde', 'oxidase'], ['for', 'Vitamin', 'D3', 'receptor', 'provide', 'actual', 'isoelectric', 'point', 'value'], ['provide', 'the', 'kingdom', 'name', 'of', 'Glutethimide'], ['provide', 'the', 'statuses', 'for', 'L', '-', 'Cysteine'], ['which', 'diseases', 'are', 'treated', 'using', 'Flurandrenolide'], ['provide', 'the', 'details', 'of', 'pharmacology', 'study', 'for', 'Dipivefrin'], ['Flavohemoprotein', 'is', 'based', 'on', 'which', 'gene'], ['the', 'actions', 'of', 'Preotact', 'focuses', 'on', 'which', 'targets'], ['what', 'are', 'the', 'synonyms', 'of', 'MGCD', '-', '0103'], ['what', 'is', 'the', 'general', 'function', 'of', 'enzme', 'Cytochrome', 'P450', '2B7', 'isoform'], ['what', 'is', 'the', 'registered', 'patent', 'number', 'of', 'Deferasirox'], ['which', 'cellular', 'location', 'we', 'can', 'find', 'enzyme', 'Aldehyde'], ['what', 'is', 'the', 'elimination', 'process', 'of', 'Ibuprofen', 'in', 'the', 'body'], ['provide', 'general', 'function', 'of', 'Transthyretin'], ['what', 'is', 'the', 'structure', 'name', 'of', 'Cefmetazole'], ['what', 'is', 'the', 'category', 'of', 'Glycine'], ['list', 'the', 'known', 'undesirable', 'side', 'effects', 'of', 'Hydromorphone'], ['Fluticasone', 'Propionate', 'is', 'protected', 'using', 'which', 'patent', 'number'], ['list', 'the', 'prescription', 'product', 'forms', 'of', 'Ketoconazole'], ['which', 'compound', 'structure', 'does', 'Phenylephrine', 'belongs', 'to'], ['which', 'salt', 'form', 'is', 'produced', 'from', 'Estriol'], ['what', 'are', 'the', 'generic', 'functionalities', 'performed', 'by', 'Acetyl', '-', 'CoA', 'carboxylase', '2'], ['provide', 'the', 'estimated', 'half', 'life', 'for', 'Fusidic', 'Acid'], ['current', 'status', 'of', 'Simvastatin', 'falls', 'under', 'which', 'groups'], ['provide', 'the', 'available', 'brands', 'for', 'Peginterferon', 'alfa', '-', '2b'], ['Loratadine', 'is', 'administered', 'to', 'treat', 'which', 'diseases'], ['what', 'is', 'the', 'working', 'mechanism', 'of', 'NBI', '-', '6024'], ['what', 'are', 'the', 'other', 'equivalent', 'names', 'used', 'for', 'Cephalexin'], ['AZD6140', 'affects', 'which', 'organisms'], ['what', 'is', 'the', 'gene', 'name', 'of', 'Vitamin', 'D', '-', 'binding', 'protein'], ['how', 'much', 'of', 'Warfarin', 'can', 'be', 'safely', 'distibuted', 'in', 'body', 'water', 'by', 'volume'], ['list', 'the', 'organisms', 'in', 'which', 'the', 'effect', 'of', 'Pegaptanib', 'is', 'observed'], ['provide', 'the', 'commercial', 'products', 'that', 'contain', 'Human', 'Serum', 'Albumin'], ['name', 'of', 'the', 'conditions', 'for', 'which', 'MEM', '1414', 'is', 'prescribed'], ['what', 'are', 'the', 'various', 'volume', 'of', 'distribution', 'of', 'Delorazepam', 'in', 'patients'], ['which', 'mixtures', 'are', 'produced', 'using', 'Menthol'], ['what', 'is', 'the', 'locus', 'value', 'of', 'enzyme', 'Cytochrome', 'P450', '2B7', 'isoform'], ['what', 'are', 'all', 'the', 'mixtures', 'made', 'with', 'Chlorphenamine'], ['Diclofenac', 'is', 'packaged', 'and', 'distributed', 'by', 'which', 'companies'], ['how', 'long', 'it', 'takes', 'for', 'Venlafaxine', 'to', 'reduce', 'its', 'concentration', 'by', 'one', 'half'], ['what', 'is', 'the', 'molecular', 'mass', 'value', 'of', 'Hydrolase'], ['list', 'the', 'constituents', 'of', 'Gold', 'bond', 'medicated', 'lotion'], ['what', 'are', 'the', 'specific', 'functions', 'carried', 'out', 'by', 'Beta', '-', 'lactamase', 'TEM'], ['list', 'drug', 'to', 'drug', 'interaction', 'for', 'Isradipine'], ['33', 'kDa', 'chaperonin', 'is', 'present', 'in', 'which', 'location', 'of', 'a', 'cell'], ['provide', 'the', 'value', 'of', 'Ammonia', 'channel', \"'\", 's', 'molecular', 'weight'], ['how', 'much', 'Moxifloxacin', 'is', 'bound', 'to', 'various', 'protein', 'types'], ['in', 'which', 'organism', 'we', 'can', 'find', 'the', 'enzyme', 'Aldehyde', 'oxidase'], ['in', 'which', 'dosage', 'form', 'Ziprasidone', 'is', 'administrated'], ['in', 'which', 'organism', 'we', 'can', 'find', 'the', 'enzyme', 'Cytochrome', 'P450', '2B7', 'isoform'], ['provide', 'locus', 'value', 'of', 'Thyroxine', '-', 'binding', 'globulin'], ['what', 'is', 'the', 'value', 'estimated', 'for', 'volume', 'of', 'distribution', 'for', 'Lepirudin'], ['which', 'company', 'produces', 'the', 'medicine', 'Clobetasol', 'propionate'], ['which', 'protein', 'helps', 'the', 'transportation', 'of', 'Ioflupane', 'I', '123', 'across', 'membrane'], ['how', 'much', 'is', 'the', 'molecular', 'weight', 'of', 'Subtilisin', 'Savinase'], ['how', 'Oxiconazole', 'acts', 'to', 'produce', 'desired', 'effects'], ['what', 'is', 'the', 'given', 'theoretical', 'pi', 'value', 'for', 'Vitamin', 'D', '-', 'binding'], ['what', 'are', 'the', 'different', 'forms', 'of', 'products', 'sold', 'for', 'Timolol'], ['provide', 'the', 'routes', 'through', 'which', 'Dobutamine', 'is', 'removed'], ['tell', 'me', 'the', 'specific', 'function', 'of', 'Thyroxine', '-', 'binding', 'globulin'], ['detail', 'the', 'actions', 'of', 'Pergolide', 'as', 'provided', 'in', 'the', 'pharmacology', 'reports'], ['which', 'specific', 'functions', 'are', 'performed', 'by', 'Peptide', 'deformylase'], ['which', 'targets', 'are', 'activated', 'by', 'Icosapent'], ['Dihomo', '-', 'γ', '-', 'linolenic', 'acid', 'is', 'avilable', 'by', 'which', 'brand', 'names'], ['at', 'which', 'locus', 'point', 'does', 'Lactotransferrin', 'present'], ['Lindane', 'is', 'classified', 'into', 'which', 'status', 'groups', 'of', 'drugs'], ['Denileukin', 'diftitox', 'is', 'cleared', 'from', 'system', 'with', 'what', 'rate'], ['provide', 'the', 'name', 'of', 'companies', 'that', 'pack', 'and', 'supply', 'Metronidazole'], ['what', 'is', 'the', 'biotransformation', 'process', 'of', 'Axitinib'], ['provide', 'the', 'name', 'of', 'the', 'transporter', 'of', 'Zidovudine'], ['which', 'gene', 'defines', 'Arylamine', 'N', '-', 'acetyltransferase'], ['provide', 'all', 'items', 'used', 'as', 'ingredient', 'for', 'Bronchyl', 'SYR'], ['list', 'Aclidinium', \"'\", 's', 'adverse', 'impacts', 'on', 'patients'], ['list', 'the', 'dose', 'forms', 'possible', 'for', 'use', 'of', '19', '-', 'norandrostenedione'], ['what', 'is', 'the', 'estimated', 'clearance', 'rate', 'values', 'for', 'Adefovir', 'Dipivoxil'], ['what', 'is', 'the', 'locus', 'position', 'of', 'Glutathione', 'peroxidase', '1', 'in', 'a', 'chromosome'], ['Chlorprothixene', 'is', 'eliminated', 'in', 'how', 'many', 'hours', 'of', 'half', 'life'], ['what', 'are', 'the', 'available', 'brand', 'names', 'for', 'Tauroursodeoxycholic', 'acid'], ['Lin', 'o', 'gel', 'is', 'made', 'using', 'which', 'ingredients'], ['provide', 'the', 'food', 'interaction', 'details', 'for', 'Ibandronate'], ['Flavopiridol', 'is', 'available', 'in', 'which', 'salt', 'forms'], ['explain', 'about', 'other', 'drugs', 'interaction', 'process', 'details', 'of', 'Magnesium', 'oxide'], ['for', 'Aldehyde', 'oxidase', 'provide', 'the', 'value', 'of', 'theoretical', 'pi'], ['DNA', 'polymerase', 'I', 'is', 'part', 'of', 'which', 'organisms'], ['provide', 'the', 'company', 'name', 'which', 'manufactures', 'Codeine'], ['provide', 'elimination', 'route', 'and', 'time', 'for', 'Clopidogrel'], ['what', 'is', 'salt', 'name', 'in', 'which', 'Ketobemidone', 'available', 'for', 'use'], ['how', 'much', 'of', 'Gadofosveset', 'trisodium', 'is', 'eliminated', 'during', 'a', 'treatment', 'in', 'unit', 'time'], ['what', 'is', 'mechanism', 'of', 'L', '-', 'Phenylalanine', 'while', 'performing', 'the', 'required', 'activities'], ['which', 'conditions', 'are', 'managed', 'with', 'help', 'of', 'Sodium', 'bicarbonate'], ['how', 'other', 'drugs', 'interact', 'with', 'drug', 'Isosorbide', 'Dinitrate'], ['in', 'which', 'forms', 'dosage', 'can', 'be', 'administrated', 'for', 'Glucagon', 'recombinant'], ['what', 'is', 'the', 'kingdom', 'of', 'Cinoxacin'], ['who', 'are', 'the', 'packagers', 'of', 'Potassium', 'Chloride'], ['what', 'are', 'the', 'known', 'food', 'interactions', 'of', 'the', 'drug', 'Cefuroxime'], ['what', 'range', 'of', 'protein', 'binding', 'happens', 'with', 'Arbekacin'], ['Cholic', 'Acid', 'addresses', 'which', 'target', 'to', 'achieve', 'the', 'desired', 'effect'], ['which', 'cellular', 'location', 'we', 'can', 'find', 'enzyme', 'Cytochrome', 'P450', '2B7', 'isoform'], ['what', 'is', 'the', 'agent', 'category', 'for', 'Cefamandole'], ['explain', 'pharmacology', 'details', 'of', 'ACA', '125'], ['Acetylsalicylic', 'acid', 'is', 'biotransformed', 'using', 'which', 'process'], ['provide', 'the', 'percentage', 'of', 'Mupirocin', 'bound', 'to', 'proteins'], ['ado', '-', 'trastuzumab', 'emtansine', 'can', 'affect', 'which', 'life', 'forms'], ['Cloxacillin', 'is', 'exisit', 'in', 'which', 'structure'], ['what', 'are', 'food', 'advices', 'for', 'adminstration', 'of', 'Isosorbide', 'Mononitrate'], ['what', 'is', 'the', 'name', 'of', 'kingdom', 'compound', 'for', 'Isosorbide', 'Dinitrate'], ['how', 'Radium', 'Ra', '223', 'Dichloride', 'is', 'transformed', 'during', 'metabolism'], ['list', 'the', 'categories', 'into', 'which', 'Thiamine', 'can', 'be', 'classified']]\n",
            "Predicted Labels\n",
            "[['B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'I-E'], ['O', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'I-E', 'I-E', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'I-E', 'B-E', 'I-E', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'B-E', 'B-E', 'B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'B-E', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'B-E', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E'], ['O', 'B-E', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O']]\n",
            "Actual Labels\n",
            "[['B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'O', 'O'], ['O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'B-E', 'B-E', 'B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'B-E', 'B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-E'], ['O', 'O', 'O', 'O', 'B-E', 'I-E'], ['B-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O'], ['B-E', 'B-E', 'B-E', 'I-E', 'O', 'O', 'O', 'O', 'O'], ['B-E', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-E', 'I-E'], ['O', 'B-E', 'I-E', 'I-E', 'I-E', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O']]\n",
            "Predicted Entities\n",
            "['Nitroglycerin', 'Phenmetrazine', 'Nepafenac', 'Hydrochlorothiazide', 'Zafirlukast', 'Theophylline', 'enzme Aldehyde oxidase', 'Vitamin D3oelectric', 'Glutethimide', 'L-Cysteine', 'Flurandrenolide', 'Dipivefrin', '', 'Preotact', 'MGCD-0103', 'enzme Cytochrome P450 2B7oform', 'Deferasirox', 'Aldehyde', 'Ibuprofen', 'Transthyretin', 'Cefmetazole', 'Glycine', 'Hydromorphone', 'Fluticasone Propionate', 'Ketoconazole', 'Phenylephrine', 'Estriol', 'Acetyl-CoA carboxylase 2', 'Fusidic Acid', 'Simvastatin', 'Peginterferon alfa-2b', '', 'NBI-6024', 'Cephalexin', '', 'Vitamin D-binding protein', 'Warfarinstibuted', 'Pegaptanib', 'Human Serum Albumin', 'MEM 1414', 'Delorazepam', 'Menthol', 'Cytochrome P450 2B7 isoform', 'Chlorphenamine', '', 'Venlafaxine reduce its concentration half', 'Hydrolase', 'Golddicated lotion', 'Beta-lactamase TEM', 'Isradipine', '', 'Ammonia channel', 'Moxifloxacin', 'Aldehyde oxidase', 'Ziprasidoneministrated', 'Cytochrome P450 2B7 isoform', 'Thyroxine-binding globulin', 'Lepirudin', 'Clobetasol propionate', 'Ioflupane I 123 across membrane', 'Subtilisin Savinase', 'Oxiconazole', 'Vitamin D-', 'Timolol', 'Dobutamine', 'Thyroxine-binding globulin', 'Pergolideharmacology', 'Peptide deformylase', 'Icosapent', '-γ-linolenic acidvilable', 'Lactotransferrin', '', 'Denileukin diftitox', 'Metronidazole', 'Axitinib', 'Zidovudine', 'Arylamine N-acetyltransferase', 'Bronchyl SYR', 'Aclidinium', '19-norandrostenedione', 'Adefovir Dipivoxil', 'Glutathione peroxidase 1', '', 'Tauroursodeoxycholic acid', 'o gel', 'Ibandronate', '', 'Magnesium oxide', 'Aldehyde oxidasei', 'polymerase I', 'Codeine', 'Clopidogrel', 'Ketobemidone', 'Gadofosveset trisodium', 'L-Phenylalanine', 'bicarbonate', 'Isosorbide Dinitrate', 'Glucagoncombinant', 'Cinoxacin', 'Potassium Chloride', 'Cefuroxime', 'Arbekacin', 'Acid', 'Cytochrome P450 2B7 isoform', 'Cefamandole', 'pharmacology ACA 125', 'acidtransformed', 'Mupirocin', '-trastuzumab emtansine', '', 'Isosorbide Mononitrate', 'Isosorbide Dinitrate', 'Radium Ra 223 Dichloride', 'Thiamine']\n",
            "Actual Entities\n",
            "['Nitroglycerin', 'Phenmetrazine', 'Nepafenac', 'Hydrochlorothiazide', 'Zafirlukast', 'Theophylline', 'Aldehyde oxidase', 'Vitamin D3 receptoroelectric', 'Glutethimide', 'L-Cysteine', 'Flurandrenolide', 'Dipivefrin', 'Flavohemoprotein', 'Preotact', 'MGCD-0103', 'Cytochrome P450 2B7 isoform', 'Deferasirox', 'enzyme Aldehyde', 'Ibuprofen', 'Transthyretin', 'Cefmetazole', 'Glycine', 'Hydromorphone', 'Fluticasone Propionate', 'Ketoconazole', 'Phenylephrine', 'Estriol', 'Acetyl-CoA carboxylase 2', 'Fusidic Acid', 'Simvastatin', 'Peginterferon alfa-2b', 'Loratadine', 'NBI-6024', 'Cephalexin', 'AZD6140', 'Vitamin D-binding protein', 'Warfarinstibuted', 'Pegaptanib', 'Human Serum Albumin', 'MEM 1414', 'Delorazepam', 'Menthol', 'Cytochrome P450 2B7 isoform', 'Chlorphenamine', 'Diclofenac', 'Venlafaxine', 'Hydrolase', 'Gold bond medicated lotion', 'Beta-lactamase TEM', 'Isradipine', '33 kDa chaperonin', 'Ammonia channel', 'Moxifloxacin', 'Aldehyde oxidase', 'Ziprasidoneministrated', 'Cytochrome P450 2B7 isoform', 'Thyroxine-binding globulin', 'Lepirudin', 'Clobetasol propionate', 'Ioflupane I 123', 'Subtilisin Savinase', 'Oxiconazole', 'for Vitamin D-binding', 'Timolol', 'Dobutamine', 'Thyroxine-binding globulin', 'Pergolideharmacology', 'Peptide deformylase', 'Icosapent', 'Dihomo-γ-linolenic acidvilable', 'Lactotransferrin', 'Lindane', 'Denileukin diftitox', 'Metronidazole', 'Axitinib', 'Zidovudine', 'Arylamine N-acetyltransferase', 'Bronchyl SYR', 'Aclidinium', '19-norandrostenedione', 'Adefovir Dipivoxil', 'Glutathione peroxidase 1', 'Chlorprothixene', 'Tauroursodeoxycholic acid', 'Lin o gel', 'Ibandronate', 'Flavopiridol', 'Magnesium oxide', 'Aldehyde oxidasei', 'DNA polymerase I', 'Codeine', 'Clopidogrel', 'Ketobemidone', 'Gadofosveset trisodium', 'L-Phenylalanine', 'Sodium bicarbonate', 'Isosorbide Dinitrate', 'Glucagon recombinant', 'Cinoxacin', 'Potassium Chloride', 'Cefuroxime', 'Arbekacin', 'Cholic Acid', 'Cytochrome P450 2B7 isoform', 'Cefamandole', 'ACA 125', 'Acetylsalicylic acidtransformed', 'Mupirocin', 'ado-trastuzumab emtansine', 'Cloxacillinisit', 'Isosorbide Mononitrate', 'Isosorbide Dinitrate', 'Radium Ra 223 Dichloride', 'Thiamine']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyrDdFSvUAEP",
        "colab_type": "text"
      },
      "source": [
        "**References**\n",
        "\n",
        "Followed Examples from\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/\n",
        "\n",
        "https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "\n",
        "http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
        "\n",
        "https://www.kaggle.com/nkaenzig/bert-tensorflow-2-huggingface-transformers\n",
        "\n",
        "https://colab.research.google.com/drive/1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU#scrollTo=tBa6vRHknSkv\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}